{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import gradio as gr\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import platform\n",
    "from ultralytics import YOLO\n",
    "import io\n",
    "import base64\n",
    "from gradio import ChatMessage\n",
    "import re\n",
    "import time\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "last_gpt_request_time = 0  # 마지막 GPT 요청 시간 기록\n",
    "gpt_request_interval = 10  # GPT 요청 간격 (초)\n",
    "model = YOLO('yolo11x.pt').to('cpu')\n",
    "SPEECH_ENDPOINT = \"https://eastus.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "SPEECH_API_KEY = \"6d450cc0d7e64602a2a9c302854fce19\"\n",
    "\n",
    "# open AI request 받아오는 함수\n",
    "def request_gpt(prompt, histories=[]):\n",
    "    ai_search_endpoint = \"https://aisearch-team8-eastus.search.windows.net\"\n",
    "    ai_search_key = \"hjEB318eEZKXt5im3PM1ejZ1HXcZfAKoO3ygcC7YVXAzSeANXLCQ\"\n",
    "    ai_search_index = \"metro2\"\n",
    "    endpoint = \"https://openai-team8-eastus2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"api-key\": \"8NSOlHOOaVXr898vFkT3m8vMuNsjYoMYwIvw0zl8uRHxok0M0Jv7JQQJ99ALACHYHv6XJ3w3AAABACOGlhnZ\"\n",
    "    }\n",
    "    \n",
    "    message_list = list()\n",
    "    \n",
    "    # 시스템 메시지\n",
    "    message_list.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"사용자가 정보를 찾는 데 도움이 되는 AI 도우미입니다.\"       \n",
    "        \n",
    "    })\n",
    "\n",
    "    # 이전 메시지 포함.\n",
    "    for history in histories:\n",
    "        for text in history:\n",
    "            message_list.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    # 유저 메시지.\n",
    "    message_list.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    })            \n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": message_list,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "        \"data_sources\": [\n",
    "        {\n",
    "            \"type\": \"azure_search\",\n",
    "            \"parameters\": {\n",
    "                \"endpoint\": ai_search_endpoint,\n",
    "                \"semantic_configuration\": \"metro2-semantic\",\n",
    "                \"query_type\": \"semantic\",\n",
    "                \"strictness\": 2,\n",
    "                \"top_n_documents\": 5,\n",
    "                \"key\": ai_search_key,\n",
    "                \"indexName\": ai_search_index\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "    }  \n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        if response_json[\"choices\"][0][\"message\"][\"context\"]:\n",
    "            citations = response_json[\"choices\"][0][\"message\"][\"context\"][\"citations\"]\n",
    "            formatted_citation_list = list()\n",
    "            i = 0\n",
    "            for c in citations:\n",
    "                i += 1\n",
    "                temp = f\"<details><summary>참조{i}</summary><ul>{c['content']}</ul></details>\"\n",
    "                formatted_citation_list.append(temp)\n",
    "\n",
    "        else:\n",
    "            # citations = \"\"\n",
    "            formatted_citation_list = list()\n",
    "        return content, \"\".join(formatted_citation_list)\n",
    "    else:\n",
    "        return f\"{response.status_code}, {response.text}\", \"\"\n",
    "\n",
    "\n",
    "def request_yolo_gpt(image_array, objects):\n",
    "    endpoint = \"https://openai-team8-eastus2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": \"8NSOlHOOaVXr898vFkT3m8vMuNsjYoMYwIvw0zl8uRHxok0M0Jv7JQQJ99ALACHYHv6XJ3w3AAABACOGlhnZ\"\n",
    "    }\n",
    "\n",
    "    # numpy 이미지를 PIL 형태로 변환\n",
    "    image = Image.fromarray(image_array)\n",
    "    buffered_io = io.BytesIO()\n",
    "    image.save(buffered_io, format='png')\n",
    "    base64_image = base64.b64encode(buffered_io.getvalue()).decode('utf-8')\n",
    "\n",
    "    object_descriptions = \"\\n\".join([\n",
    "    f\"{obj[0]} ({obj[2]*100:.2f}%) at {obj[1]}\" for obj in objects\n",
    "])\n",
    "    \n",
    "    user_message = f\"\"\"\n",
    "    너는 물체를 감지해서 시각장애인에게 앞에 무엇이 있는지 위험을 알려주는 YOLO 모델이야.\n",
    "    감지된 물체는 다음과 같아:\n",
    "    {object_descriptions}\n",
    "    이 사진에서 바운딩 박스에 감지된 물체에 대해 왜 위험한지 간단하게 설명해줘.\n",
    "    \"\"\"\n",
    "\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"너는 사진 속에서 감지된 물체에 대해 위험성을 분석하는 봇이야.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    message_list.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": user_message\n",
    "        },{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": message_list,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 1500\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        return response_json['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return \"Error: Unable to process your request.\"\n",
    "    \n",
    "def request_tts(text):\n",
    "    endpoint = SPEECH_ENDPOINT\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": SPEECH_API_KEY,\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\": \"audio-16khz-128kbitrate-mono-mp3\"\n",
    "    }\n",
    "    payload = f\"\"\"\n",
    "    <speak version='1.0' xml:lang='en-US'>\n",
    "        <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "            {text}\n",
    "        </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_name = \"response_audio.mp3\"\n",
    "        print(response.status_code)\n",
    "        with open(file_name, \"wb\") as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def detect_objects(origin_image):\n",
    "    image = origin_image.copy()\n",
    "    image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    response = model(origin_image)\n",
    "\n",
    "    font_size = 15\n",
    "    if platform.system() == \"Darwin\":\n",
    "        font = ImageFont.truetype(\"AppleGothic.ttf\", size=font_size)\n",
    "    elif platform.system() == \"Windows\":\n",
    "        font = ImageFont.truetype(\"malgun.ttf\", size=font_size)\n",
    "    else:\n",
    "        font = ImageFont.load_default(size=font_size)\n",
    "\n",
    "    filtered_results = []  # 필터링된 결과 저장\n",
    "\n",
    "    for result in response:\n",
    "        label_list = result.names\n",
    "        box_list = result.boxes.xyxy.cpu().numpy()\n",
    "        confidence_list = result.boxes.conf.cpu().numpy()\n",
    "        class_id_list = result.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for index, box in enumerate(box_list):\n",
    "            confidence = confidence_list[index]\n",
    "            class_id = class_id_list[index]\n",
    "            label = label_list[class_id]\n",
    "\n",
    "            # 라벨링 데이터만 필터링 (예: 신뢰도 50% 이상만 포함)\n",
    "            if confidence >= 0.5:  # 조건에 따라 필터링 가능\n",
    "                filtered_results.append((label, box, confidence))\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                draw.rectangle((x1, y1, x2, y2), outline=(255, 217, 0), width=2)\n",
    "                draw.text((x1 + 5, y1 + 5), text=f\"{label} ({(confidence * 100):.2f}%)\", fill=(255, 217, 0), font=font)\n",
    "\n",
    "    return image, filtered_results  # 이미지와 필터링된 결과 반환\n",
    "\n",
    "def request_stt_fast(file_path):\n",
    "    endpoint = \"https://eastus.api.cognitive.microsoft.com/speechtotext/transcriptions:transcribe?api-version=2024-11-15\"\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": \"6d450cc0d7e64602a2a9c302854fce19\",\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"rb\") as audio :\n",
    "        audio_data = audio.read()\n",
    "\n",
    "    json = {\n",
    "        \"definition\": '{\"locales\":[\"ko-KR\"], \"profanityFilterMode\":\"Masked\", \"channels\":[0,1]}'\n",
    "    }\n",
    "\n",
    "    files = {\n",
    "        \"audio\" : audio_data\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers,files=files, json=json)\n",
    "\n",
    "    print(\"Fast\", response.status_code, response.elapsed.total_seconds())\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        response_text = response_json['combinedPhrases'][0]['text']\n",
    "        print(response_text)\n",
    "        return response_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def change_chatbot(histories):\n",
    "    content = histories[-1]['content']\n",
    "\n",
    "    pattern = r'[^가-힣a-zA-Z0-9\\s%,\\.]'\n",
    "    cleaned_text = re.sub(pattern, '', content)\n",
    "    file_name = request_tts(cleaned_text)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "chat_history = []  # 채팅 히스토리 기록\n",
    "\n",
    "def stream_webcam(image):\n",
    "    global last_gpt_request_time, chat_history\n",
    "    current_time = time.time()\n",
    "\n",
    "    # 객체 감지\n",
    "    detected_image, detected_objects = detect_objects(image)\n",
    "\n",
    "    # GPT 요청 주기적 실행\n",
    "    if current_time - last_gpt_request_time > gpt_request_interval:\n",
    "        last_gpt_request_time = current_time\n",
    "        gpt_response = request_yolo_gpt(image_array=image, objects=detected_objects)\n",
    "\n",
    "        # 채팅 기록에 GPT 응답 추가\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": gpt_response})\n",
    "\n",
    "    return detected_image, chat_history\n",
    "\n",
    "def change_audio(file_path):\n",
    "    if file_path:\n",
    "        response_text = request_stt_fast(file_path=file_path)\n",
    "        return response_text\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def click_send(prompt, histories):\n",
    "    response_text, citation_html = request_gpt(prompt=prompt, histories=histories)\n",
    "    histories.append((prompt, response_text))\n",
    "    \n",
    "    pattern = r'[^가-힣a-zA-Z0-9\\s]'\n",
    "    formatted_response_text = re.sub(pattern, '', response_text)\n",
    "    \n",
    "    audio_file_name = request_tts(formatted_response_text)\n",
    "    return histories, audio_file_name, citation_html\n",
    "\n",
    "def synthesize_speech(text):\n",
    "    speech_key = \"4cc73b76c3984f13a64e8230fc8b5d8d\" \n",
    "    region = \"eastus\"\n",
    "    \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)  # 기본 스피커로 음성 출력\n",
    "\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    synthesizer.speak_text_async(text) \n",
    "\n",
    "def read_tab_label(tab_label):\n",
    "        \"\"\"탭의 라벨을 읽어서 TTS로 출력\"\"\"\n",
    "        synthesize_speech(tab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://fa751d4f02d613961f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fa751d4f02d613961f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 3380.2ms\n",
      "Speed: 11.5ms preprocess, 3380.2ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "200\n",
      "0: 384x640 1 person, 2673.2ms\n",
      "Speed: 3.9ms preprocess, 2673.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "200\n",
      "0: 384x640 1 person, 4530.8ms\n",
      "Speed: 7.7ms preprocess, 4530.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3077.8ms\n",
      "Speed: 3.4ms preprocess, 3077.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Tab(label=\"위험 감지\") as tab1 :\n",
    "        gr.Markdown(\"## 📸 위험 감지 화면\")\n",
    "\n",
    "        # 실시간 화면과 검출 화면을 세로 배치\n",
    "        with gr.Column():\n",
    "            input_webcam = gr.Image(\n",
    "                label=\"실시간 화면\",\n",
    "                sources=\"webcam\",\n",
    "                height=250,  # 모바일 화면 최적화\n",
    "                width=\"100%\",\n",
    "                mirror_webcam=False\n",
    "            )\n",
    "\n",
    "        gr.Markdown(\"### 검출 화면 및 분석 결과\")\n",
    "        with gr.Row():\n",
    "            output_image = gr.Image(\n",
    "                label=\"검출 화면\",\n",
    "                type=\"pil\",\n",
    "                interactive=False,\n",
    "                height=220,\n",
    "                width=\"50%\"\n",
    "            )\n",
    "\n",
    "            chatbot_yolo = gr.Chatbot(\n",
    "                label=\"⚠️ 분석 결과\",\n",
    "                height=220,  # 모바일 화면에 맞는 높이\n",
    "                type=\"messages\"\n",
    "            )          \n",
    "\n",
    "        # 분석 결과\n",
    "        with gr.Column():\n",
    "\n",
    "            gr.Markdown(\"### 🔊 분석 오디오\")\n",
    "            chatbot_audio_yolo = gr.Audio(\n",
    "                label=\"🔊 분석 오디오\",\n",
    "                interactive=False,\n",
    "                autoplay=True, waveform_options=gr.WaveformOptions(\n",
    "                    waveform_color=\"#D3B1C2\",\n",
    "                    waveform_progress_color=\"#C197D@\",\n",
    "                    skip_length=2,\n",
    "                    show_controls=False))\n",
    "\n",
    "        # 스트림 함수 연결\n",
    "        input_webcam.stream(\n",
    "            fn=stream_webcam,\n",
    "            inputs=[input_webcam],\n",
    "            outputs=[output_image, chatbot_yolo]\n",
    "        )\n",
    "        chatbot_yolo.change(fn=change_chatbot, inputs=[chatbot_yolo], outputs=[chatbot_audio_yolo])\n",
    "\n",
    "    with gr.Tab(label=\"교통 정보\")  as tab2:\n",
    "        gr.Markdown(\"## 🚦 교통 정보\")\n",
    "\n",
    "        # 음성 입력 및 결과 출력\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 🎤 마이크 입력\")\n",
    "            input_mic = gr.Audio(\n",
    "                label=\"🎤 마이크 입력\",\n",
    "                sources=\"microphone\",\n",
    "                type=\"filepath\",\n",
    "                interactive=True, waveform_options=gr.WaveformOptions(\n",
    "                    waveform_color=\"#D3B1C2\",\n",
    "                    waveform_progress_color=\"#C197D@\",\n",
    "                    skip_length=2,\n",
    "                    show_controls=False)\n",
    "            )\n",
    "            input_openai_textbox = gr.Textbox(\n",
    "                label=\"📝 텍스트\", \n",
    "                interactive=False, \n",
    "                placeholder=\"음성 입력 결과가 여기에 표시됩니다.\"\n",
    "            )\n",
    "\n",
    "        # 프롬프트 입력과 결과\n",
    "        gr.Markdown(\"### 💬 대화 내용\")\n",
    "        chatbot_gpt = gr.Chatbot(\n",
    "            label=\"💬 대화 내용\", \n",
    "            height=200\n",
    "        )\n",
    "        gr.Markdown(\"### 🔊 GPT 음성 응답\")\n",
    "        chatbot_audio_gpt = gr.Audio(\n",
    "            label=\"🔊 GPT 음성 응답\", \n",
    "            interactive=False,\n",
    "            autoplay=True, \n",
    "        )\n",
    "        citation = gr.HTML(label=\"🔗 참조 링크\")\n",
    "\n",
    "        # 이벤트 연결\n",
    "        input_mic.change(\n",
    "            fn=change_audio, \n",
    "            inputs=[input_mic], \n",
    "            outputs=[input_openai_textbox]\n",
    "        )\n",
    "        input_openai_textbox.change(\n",
    "            fn=click_send,\n",
    "            inputs=[input_openai_textbox, chatbot_gpt],\n",
    "            outputs=[chatbot_gpt, chatbot_audio_gpt, citation]\n",
    "        )\n",
    "        \n",
    "        # 페이지 로드시 기본으로 위험 감지 값을 가지고 tts로 출력\n",
    "    demo.load(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"위험 감지\"),  # 초기 탭 라벨 설정\n",
    "            )\n",
    "    # 탭 선택 시 어떤 탭인지 tts 출력\n",
    "    tab1.select(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"위험 감지\")\n",
    "               )\n",
    "    tab2.select(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"교통 정보\")\n",
    "               )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
