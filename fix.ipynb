{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import gradio as gr\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import platform\n",
    "from ultralytics import YOLO\n",
    "import io\n",
    "import base64\n",
    "from gradio import ChatMessage\n",
    "import re\n",
    "import time\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "last_gpt_request_time = 0  # ë§ˆì§€ë§‰ GPT ìš”ì²­ ì‹œê°„ ê¸°ë¡\n",
    "gpt_request_interval = 10  # GPT ìš”ì²­ ê°„ê²© (ì´ˆ)\n",
    "model = YOLO('yolo11x.pt').to('cpu')\n",
    "SPEECH_ENDPOINT = \"https://eastus.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "SPEECH_API_KEY = \"6d450cc0d7e64602a2a9c302854fce19\"\n",
    "\n",
    "# open AI request ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def request_gpt(prompt, histories=[]):\n",
    "    ai_search_endpoint = \"https://aisearch-team8-eastus.search.windows.net\"\n",
    "    ai_search_key = \"hjEB318eEZKXt5im3PM1ejZ1HXcZfAKoO3ygcC7YVXAzSeANXLCQ\"\n",
    "    ai_search_index = \"metro2\"\n",
    "    endpoint = \"https://openai-team8-eastus2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"api-key\": \"8NSOlHOOaVXr898vFkT3m8vMuNsjYoMYwIvw0zl8uRHxok0M0Jv7JQQJ99ALACHYHv6XJ3w3AAABACOGlhnZ\"\n",
    "    }\n",
    "    \n",
    "    message_list = list()\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ ë©”ì‹œì§€\n",
    "    message_list.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ì‚¬ìš©ìê°€ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\"       \n",
    "        \n",
    "    })\n",
    "\n",
    "    # ì´ì „ ë©”ì‹œì§€ í¬í•¨.\n",
    "    for history in histories:\n",
    "        for text in history:\n",
    "            message_list.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "    # ìœ ì € ë©”ì‹œì§€.\n",
    "    message_list.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    })            \n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": message_list,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "        \"data_sources\": [\n",
    "        {\n",
    "            \"type\": \"azure_search\",\n",
    "            \"parameters\": {\n",
    "                \"endpoint\": ai_search_endpoint,\n",
    "                \"semantic_configuration\": \"metro2-semantic\",\n",
    "                \"query_type\": \"semantic\",\n",
    "                \"strictness\": 2,\n",
    "                \"top_n_documents\": 5,\n",
    "                \"key\": ai_search_key,\n",
    "                \"indexName\": ai_search_index\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "    }  \n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        if response_json[\"choices\"][0][\"message\"][\"context\"]:\n",
    "            citations = response_json[\"choices\"][0][\"message\"][\"context\"][\"citations\"]\n",
    "            formatted_citation_list = list()\n",
    "            i = 0\n",
    "            for c in citations:\n",
    "                i += 1\n",
    "                temp = f\"<details><summary>ì°¸ì¡°{i}</summary><ul>{c['content']}</ul></details>\"\n",
    "                formatted_citation_list.append(temp)\n",
    "\n",
    "        else:\n",
    "            # citations = \"\"\n",
    "            formatted_citation_list = list()\n",
    "        return content, \"\".join(formatted_citation_list)\n",
    "    else:\n",
    "        return f\"{response.status_code}, {response.text}\", \"\"\n",
    "\n",
    "\n",
    "def request_yolo_gpt(image_array, objects):\n",
    "    endpoint = \"https://openai-team8-eastus2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": \"8NSOlHOOaVXr898vFkT3m8vMuNsjYoMYwIvw0zl8uRHxok0M0Jv7JQQJ99ALACHYHv6XJ3w3AAABACOGlhnZ\"\n",
    "    }\n",
    "\n",
    "    # numpy ì´ë¯¸ì§€ë¥¼ PIL í˜•íƒœë¡œ ë³€í™˜\n",
    "    image = Image.fromarray(image_array)\n",
    "    buffered_io = io.BytesIO()\n",
    "    image.save(buffered_io, format='png')\n",
    "    base64_image = base64.b64encode(buffered_io.getvalue()).decode('utf-8')\n",
    "\n",
    "    object_descriptions = \"\\n\".join([\n",
    "    f\"{obj[0]} ({obj[2]*100:.2f}%) at {obj[1]}\" for obj in objects\n",
    "])\n",
    "    \n",
    "    user_message = f\"\"\"\n",
    "    ë„ˆëŠ” ë¬¼ì²´ë¥¼ ê°ì§€í•´ì„œ ì‹œê°ì¥ì• ì¸ì—ê²Œ ì•ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ ìœ„í—˜ì„ ì•Œë ¤ì£¼ëŠ” YOLO ëª¨ë¸ì´ì•¼.\n",
    "    ê°ì§€ëœ ë¬¼ì²´ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n",
    "    {object_descriptions}\n",
    "    ì´ ì‚¬ì§„ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ì— ê°ì§€ëœ ë¬¼ì²´ì— ëŒ€í•´ ì™œ ìœ„í—˜í•œì§€ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤˜.\n",
    "    \"\"\"\n",
    "\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"ë„ˆëŠ” ì‚¬ì§„ ì†ì—ì„œ ê°ì§€ëœ ë¬¼ì²´ì— ëŒ€í•´ ìœ„í—˜ì„±ì„ ë¶„ì„í•˜ëŠ” ë´‡ì´ì•¼.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    message_list.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": user_message\n",
    "        },{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": message_list,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 1500\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        return response_json['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return \"Error: Unable to process your request.\"\n",
    "    \n",
    "def request_tts(text):\n",
    "    endpoint = SPEECH_ENDPOINT\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": SPEECH_API_KEY,\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\": \"audio-16khz-128kbitrate-mono-mp3\"\n",
    "    }\n",
    "    payload = f\"\"\"\n",
    "    <speak version='1.0' xml:lang='en-US'>\n",
    "        <voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-SunHiNeural'>\n",
    "            {text}\n",
    "        </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.post(endpoint, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_name = \"response_audio.mp3\"\n",
    "        print(response.status_code)\n",
    "        with open(file_name, \"wb\") as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def detect_objects(origin_image):\n",
    "    image = origin_image.copy()\n",
    "    image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    response = model(origin_image)\n",
    "\n",
    "    font_size = 15\n",
    "    if platform.system() == \"Darwin\":\n",
    "        font = ImageFont.truetype(\"AppleGothic.ttf\", size=font_size)\n",
    "    elif platform.system() == \"Windows\":\n",
    "        font = ImageFont.truetype(\"malgun.ttf\", size=font_size)\n",
    "    else:\n",
    "        font = ImageFont.load_default(size=font_size)\n",
    "\n",
    "    filtered_results = []  # í•„í„°ë§ëœ ê²°ê³¼ ì €ì¥\n",
    "\n",
    "    for result in response:\n",
    "        label_list = result.names\n",
    "        box_list = result.boxes.xyxy.cpu().numpy()\n",
    "        confidence_list = result.boxes.conf.cpu().numpy()\n",
    "        class_id_list = result.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for index, box in enumerate(box_list):\n",
    "            confidence = confidence_list[index]\n",
    "            class_id = class_id_list[index]\n",
    "            label = label_list[class_id]\n",
    "\n",
    "            # ë¼ë²¨ë§ ë°ì´í„°ë§Œ í•„í„°ë§ (ì˜ˆ: ì‹ ë¢°ë„ 50% ì´ìƒë§Œ í¬í•¨)\n",
    "            if confidence >= 0.5:  # ì¡°ê±´ì— ë”°ë¼ í•„í„°ë§ ê°€ëŠ¥\n",
    "                filtered_results.append((label, box, confidence))\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                draw.rectangle((x1, y1, x2, y2), outline=(255, 217, 0), width=2)\n",
    "                draw.text((x1 + 5, y1 + 5), text=f\"{label} ({(confidence * 100):.2f}%)\", fill=(255, 217, 0), font=font)\n",
    "\n",
    "    return image, filtered_results  # ì´ë¯¸ì§€ì™€ í•„í„°ë§ëœ ê²°ê³¼ ë°˜í™˜\n",
    "\n",
    "def request_stt_fast(file_path):\n",
    "    endpoint = \"https://eastus.api.cognitive.microsoft.com/speechtotext/transcriptions:transcribe?api-version=2024-11-15\"\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": \"6d450cc0d7e64602a2a9c302854fce19\",\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"rb\") as audio :\n",
    "        audio_data = audio.read()\n",
    "\n",
    "    json = {\n",
    "        \"definition\": '{\"locales\":[\"ko-KR\"], \"profanityFilterMode\":\"Masked\", \"channels\":[0,1]}'\n",
    "    }\n",
    "\n",
    "    files = {\n",
    "        \"audio\" : audio_data\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers,files=files, json=json)\n",
    "\n",
    "    print(\"Fast\", response.status_code, response.elapsed.total_seconds())\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        response_text = response_json['combinedPhrases'][0]['text']\n",
    "        print(response_text)\n",
    "        return response_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def change_chatbot(histories):\n",
    "    content = histories[-1]['content']\n",
    "\n",
    "    pattern = r'[^ê°€-í£a-zA-Z0-9\\s%,\\.]'\n",
    "    cleaned_text = re.sub(pattern, '', content)\n",
    "    file_name = request_tts(cleaned_text)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "chat_history = []  # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸°ë¡\n",
    "\n",
    "def stream_webcam(image):\n",
    "    global last_gpt_request_time, chat_history\n",
    "    current_time = time.time()\n",
    "\n",
    "    # ê°ì²´ ê°ì§€\n",
    "    detected_image, detected_objects = detect_objects(image)\n",
    "\n",
    "    # GPT ìš”ì²­ ì£¼ê¸°ì  ì‹¤í–‰\n",
    "    if current_time - last_gpt_request_time > gpt_request_interval:\n",
    "        last_gpt_request_time = current_time\n",
    "        gpt_response = request_yolo_gpt(image_array=image, objects=detected_objects)\n",
    "\n",
    "        # ì±„íŒ… ê¸°ë¡ì— GPT ì‘ë‹µ ì¶”ê°€\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": gpt_response})\n",
    "\n",
    "    return detected_image, chat_history\n",
    "\n",
    "def change_audio(file_path):\n",
    "    if file_path:\n",
    "        response_text = request_stt_fast(file_path=file_path)\n",
    "        return response_text\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def click_send(prompt, histories):\n",
    "    response_text, citation_html = request_gpt(prompt=prompt, histories=histories)\n",
    "    histories.append((prompt, response_text))\n",
    "    \n",
    "    pattern = r'[^ê°€-í£a-zA-Z0-9\\s]'\n",
    "    formatted_response_text = re.sub(pattern, '', response_text)\n",
    "    \n",
    "    audio_file_name = request_tts(formatted_response_text)\n",
    "    return histories, audio_file_name, citation_html\n",
    "\n",
    "def synthesize_speech(text):\n",
    "    speech_key = \"4cc73b76c3984f13a64e8230fc8b5d8d\" \n",
    "    region = \"eastus\"\n",
    "    \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)  # ê¸°ë³¸ ìŠ¤í”¼ì»¤ë¡œ ìŒì„± ì¶œë ¥\n",
    "\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    synthesizer.speak_text_async(text) \n",
    "\n",
    "def read_tab_label(tab_label):\n",
    "        \"\"\"íƒ­ì˜ ë¼ë²¨ì„ ì½ì–´ì„œ TTSë¡œ ì¶œë ¥\"\"\"\n",
    "        synthesize_speech(tab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://fa751d4f02d613961f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fa751d4f02d613961f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 3380.2ms\n",
      "Speed: 11.5ms preprocess, 3380.2ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "200\n",
      "0: 384x640 1 person, 2673.2ms\n",
      "Speed: 3.9ms preprocess, 2673.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "200\n",
      "0: 384x640 1 person, 4530.8ms\n",
      "Speed: 7.7ms preprocess, 4530.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3077.8ms\n",
      "Speed: 3.4ms preprocess, 3077.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Tab(label=\"ìœ„í—˜ ê°ì§€\") as tab1 :\n",
    "        gr.Markdown(\"## ğŸ“¸ ìœ„í—˜ ê°ì§€ í™”ë©´\")\n",
    "\n",
    "        # ì‹¤ì‹œê°„ í™”ë©´ê³¼ ê²€ì¶œ í™”ë©´ì„ ì„¸ë¡œ ë°°ì¹˜\n",
    "        with gr.Column():\n",
    "            input_webcam = gr.Image(\n",
    "                label=\"ì‹¤ì‹œê°„ í™”ë©´\",\n",
    "                sources=\"webcam\",\n",
    "                height=250,  # ëª¨ë°”ì¼ í™”ë©´ ìµœì í™”\n",
    "                width=\"100%\",\n",
    "                mirror_webcam=False\n",
    "            )\n",
    "\n",
    "        gr.Markdown(\"### ê²€ì¶œ í™”ë©´ ë° ë¶„ì„ ê²°ê³¼\")\n",
    "        with gr.Row():\n",
    "            output_image = gr.Image(\n",
    "                label=\"ê²€ì¶œ í™”ë©´\",\n",
    "                type=\"pil\",\n",
    "                interactive=False,\n",
    "                height=220,\n",
    "                width=\"50%\"\n",
    "            )\n",
    "\n",
    "            chatbot_yolo = gr.Chatbot(\n",
    "                label=\"âš ï¸ ë¶„ì„ ê²°ê³¼\",\n",
    "                height=220,  # ëª¨ë°”ì¼ í™”ë©´ì— ë§ëŠ” ë†’ì´\n",
    "                type=\"messages\"\n",
    "            )          \n",
    "\n",
    "        # ë¶„ì„ ê²°ê³¼\n",
    "        with gr.Column():\n",
    "\n",
    "            gr.Markdown(\"### ğŸ”Š ë¶„ì„ ì˜¤ë””ì˜¤\")\n",
    "            chatbot_audio_yolo = gr.Audio(\n",
    "                label=\"ğŸ”Š ë¶„ì„ ì˜¤ë””ì˜¤\",\n",
    "                interactive=False,\n",
    "                autoplay=True, waveform_options=gr.WaveformOptions(\n",
    "                    waveform_color=\"#D3B1C2\",\n",
    "                    waveform_progress_color=\"#C197D@\",\n",
    "                    skip_length=2,\n",
    "                    show_controls=False))\n",
    "\n",
    "        # ìŠ¤íŠ¸ë¦¼ í•¨ìˆ˜ ì—°ê²°\n",
    "        input_webcam.stream(\n",
    "            fn=stream_webcam,\n",
    "            inputs=[input_webcam],\n",
    "            outputs=[output_image, chatbot_yolo]\n",
    "        )\n",
    "        chatbot_yolo.change(fn=change_chatbot, inputs=[chatbot_yolo], outputs=[chatbot_audio_yolo])\n",
    "\n",
    "    with gr.Tab(label=\"êµí†µ ì •ë³´\")  as tab2:\n",
    "        gr.Markdown(\"## ğŸš¦ êµí†µ ì •ë³´\")\n",
    "\n",
    "        # ìŒì„± ì…ë ¥ ë° ê²°ê³¼ ì¶œë ¥\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### ğŸ¤ ë§ˆì´í¬ ì…ë ¥\")\n",
    "            input_mic = gr.Audio(\n",
    "                label=\"ğŸ¤ ë§ˆì´í¬ ì…ë ¥\",\n",
    "                sources=\"microphone\",\n",
    "                type=\"filepath\",\n",
    "                interactive=True, waveform_options=gr.WaveformOptions(\n",
    "                    waveform_color=\"#D3B1C2\",\n",
    "                    waveform_progress_color=\"#C197D@\",\n",
    "                    skip_length=2,\n",
    "                    show_controls=False)\n",
    "            )\n",
    "            input_openai_textbox = gr.Textbox(\n",
    "                label=\"ğŸ“ í…ìŠ¤íŠ¸\", \n",
    "                interactive=False, \n",
    "                placeholder=\"ìŒì„± ì…ë ¥ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.\"\n",
    "            )\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ ì…ë ¥ê³¼ ê²°ê³¼\n",
    "        gr.Markdown(\"### ğŸ’¬ ëŒ€í™” ë‚´ìš©\")\n",
    "        chatbot_gpt = gr.Chatbot(\n",
    "            label=\"ğŸ’¬ ëŒ€í™” ë‚´ìš©\", \n",
    "            height=200\n",
    "        )\n",
    "        gr.Markdown(\"### ğŸ”Š GPT ìŒì„± ì‘ë‹µ\")\n",
    "        chatbot_audio_gpt = gr.Audio(\n",
    "            label=\"ğŸ”Š GPT ìŒì„± ì‘ë‹µ\", \n",
    "            interactive=False,\n",
    "            autoplay=True, \n",
    "        )\n",
    "        citation = gr.HTML(label=\"ğŸ”— ì°¸ì¡° ë§í¬\")\n",
    "\n",
    "        # ì´ë²¤íŠ¸ ì—°ê²°\n",
    "        input_mic.change(\n",
    "            fn=change_audio, \n",
    "            inputs=[input_mic], \n",
    "            outputs=[input_openai_textbox]\n",
    "        )\n",
    "        input_openai_textbox.change(\n",
    "            fn=click_send,\n",
    "            inputs=[input_openai_textbox, chatbot_gpt],\n",
    "            outputs=[chatbot_gpt, chatbot_audio_gpt, citation]\n",
    "        )\n",
    "        \n",
    "        # í˜ì´ì§€ ë¡œë“œì‹œ ê¸°ë³¸ìœ¼ë¡œ ìœ„í—˜ ê°ì§€ ê°’ì„ ê°€ì§€ê³  ttsë¡œ ì¶œë ¥\n",
    "    demo.load(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"ìœ„í—˜ ê°ì§€\"),  # ì´ˆê¸° íƒ­ ë¼ë²¨ ì„¤ì •\n",
    "            )\n",
    "    # íƒ­ ì„ íƒ ì‹œ ì–´ë–¤ íƒ­ì¸ì§€ tts ì¶œë ¥\n",
    "    tab1.select(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"ìœ„í—˜ ê°ì§€\")\n",
    "               )\n",
    "    tab2.select(\n",
    "        fn=read_tab_label,\n",
    "        inputs=gr.State(value=\"êµí†µ ì •ë³´\")\n",
    "               )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
